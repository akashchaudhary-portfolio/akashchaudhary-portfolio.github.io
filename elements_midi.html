<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>GUI details</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Akash Chaudhary'S Portfolio</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="assets/Akash_Chaudhary_CV.pdf">CV</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="class_main" class="wrapper">
						<div class="inner">
							<h1 class="major">Data Sonification</h1>

							<!-- Text -->
								

							

							<!-- Image -->
								<section>
									<p style="color:rgba(255, 255, 255); font-size: 22px">Hobby project - Common Cross-Modal MIDI Toolbox</p>
									<p style="wcolor:rgba(255, 255, 255, 0.55); font-size: 22px">Python-based Toolbox for Converting, Editing and Transfering MIDI</p>
									
									<div class="box alt">
										<div class="row gtr-uniform">
											<div style = "width: 80%; margin: auto" class="col-12"><span class="image fit"><img src="images/Screenshot (78).png" alt="" /></span></div>
										</div>
									</div>
									<hr/>
									<br>
									<br>

									<h2>Summary</h2>
									<p><span class="image right"><img src="images/IMG_1590.jpg" alt="" /></span>As a team, we wanted to build an audio interface for enabling MIDI-based creative transformations. We developed a toolbox to enable artists in converting different types of files to MIDI messages, while giving them freedom to transform and manipulate them using the feature set on the GUI. These MIDI are then, sent to an output channel for interaction with various related software, such as SuperCollider and  Reaper. The relevance of such a graphical user interface was understood after realizing the limitations in the existing digital audio workstations. Future iterations of the work will have artificial intelligence embedded capabilities for assisting music creation.
									<br>
									<br>
									<br>
									<hr/>
									<br>
									<br>

									
									<h2>My responsibilities</h2>
									<ol>
										<div style="width: 45%; float:left">
										<li>I was responsible for working on audio modality in the system.</li>
										<li>I was directly involved in expanding the control given to users in terms of introducing innovative pixel operations and gestures like mouse swipe up, mouse swipe down etc. on input image files through the GUI.</li>
										<li>I designed a wide list of output parameters in audio like volume, melody, mood(dissonant or consonant decisions), panning, scale, pitch, keys, stylistic ranges and transfers that were embedded in the system for achieving the creative process.</li>
										<li>I came up with new ideas to separate the components of an audio file, through new algorithms like blind source separation.</li>
										</div>

										<div style="width: 45%; float:right">
										<li>I, along with my team designed the user study for testing the aesthetics in the output creation of the system and measuring the creative capability of the system keeping in mind the operator and data biases prevalent in such systems.</li>
										<li>I learnt non-linear mappings between projected images and audio samples.</li>
										<li>I developed linear and non-linear envelopes on data filters along with other common features present in DAWs.</li>
										<li>I prepared the team for the development of a prospective real-time output system which would involve integrating generative artificial intelligence at the backend.</li>
										</div>
									</ol>

									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									<br>
									
									<p>I worked on this project with my excellent and technically proficient ILIAD lab teammates <a href="https://in.linkedin.com/in/brihijoshi/?originalSubdomain=in">Brihi Joshi</a> (pursuing B.Tech in IIIT-Delhi), <a href="https://in.linkedin.com/in/aditya-adhikary-978861111/?originalSubdomain=in">Aditya Adhikary</a> (pursuing B.Tech in IIIT-Delhi) and <a href="https://www.linkedin.com/in/prashant-sharma-a5b594112/?originalSubdomain=in">Prashant Sharma</a> (pursuing B.Tech in IIIT-Delhi). Brihi was responsible for working on image files, Aditya worked on video files, Prashant worked on csv files and I worked on audio files. <a href="http://www.timmoyers.com/bio.html">Dr. Timothy Scott Moyers Jr.</a> was the advisor for the work.</p>

									<hr/>
									<br>
									<br>
									

									<h2>Graphical user interface</h2>
									<p><span class="image right"><img src="images/tim/a.jpg" alt="" /></span>We realized the need for a system that enables MIDI manipulations for artists working on various DAWs and VSTs. We wanted to create a backbone of MIDI or OSC messages that artists can have a control of while making digital transformations. We developed an interactive toolset for seamless data sonification from various types of file formats like video, image, audio and csv file types. The MIDI or OSC messages obtained on the GUI, are made available for manipulation by users/artists using the provided transformations on the feature set in the GUI. Finally, they are sent for communication with different audio, visual and related software (such as SuperCollider and  Reaper). We used this to develope a pipeline for bridging Audio and Visual Engines with real-time modifications. </p>

									
									</section>

									<hr/>
									<p><span class="image left"><img src="images/tim/b.jpg" alt="" /></span>In the first phase of the system, the images are MIDI extracted and transformed according to a set of operator-defined instructions. These patterns are then sent through our output channels to create music using output mediums like SuperCollider and Reaper. Different types of images expressing different moods or intentionalities, for example, anger and happiness are fed and perceivable differences are measured across all users. We wanted to find parameters of the input image corresponding to the output audio that represent an input-output pattern across different users, different fed data and different input user operations. The system is designed such that users are given as much control in the creative process that enables them to relate to the output changes according to the input actions fed by them. These include the type of input fed, the rate at which the input is fed and the duration for which input is fed. We aim to create a system where, if the user makes a change in the system, there is an obvious change in the output i.e. there is a discernable way to learn the processes that aides the music creation process. The second phase of the system is responsible for generating novel patterns that complement the output audio structure of the first phase. This is still ongoing and will embed artificial intelligence assisted backend for more creative output solutions. The system iteratively tries to learn the output on the parameters set by the user and the operations defined by the user in the GUI.</p>
									<!-- <hr/> -->

									<!-- <p><span class="image right"><img src="images/tim/c.jpg" alt="" /></span>The next iteration of the system is supposed to provide real-time or semi real-time audio outputs on the inputs received in the previous phase of the system. In this phase, a user feeds different types of audio files like spoken words, instrument sounds like percussion, musical songs, and noise patterns. This is done to adjust the system sensitivity in creating new patterns according to the type of input audio. While the system is supposed to provide intelligent and adaptive AI-generated cues for the next sequence of generated audio files, the user has the option of manipulating the input in real-time by providing different input patterns through manipulations in input audio files, audio recordings and touch gestures on image files. The last phase of the work will embed video related input capabilities to exact noticeable changes in output audio with the change in video.</p> -->
									
									<hr />
									<br>
									<br>

									<h3>Member, ILIAD Lab, IIIT - Delhi</h3>
									<p><span class="image left"><img src="images/iliad_2019.jpeg.jpg" alt="" /></span>ILIAD Lab stands for Interdisciplinary Lab for Interactive AV Development. The name has been adopted from a Greek poem attributed to Homer. This is the creative and audio lab at IIIT-Delhi and I am a member of the team working under Dr Timothy Scott Moyers Jr(Tim). Tim has a philosophical and innovative side to his personality and we usually have on the walk discussions with him regarding the universal patterns in music. We also get a taste of the different types of music (and noise) which are and have remained prevalent, and the great musicians that have flourished throughout history when we are with him. It's a lab for fun and creativity with no deadlines for any work. Everyone just loves the company of each other, and find themselves involved in work together. We build toolkits for people that help in enabling creative capabilities for artists.</p>
									<hr/>

								<section>
									<p style="font-size:90%"><a href="https://github.com/iliad-iiit-delhi">https://github.com/iliad-iiit-delhi</a></p>
									
									

								</section>	

								
						</div>
					</section>

			</div>

		<!-- Footer -->
			<!-- <footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer> -->

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>